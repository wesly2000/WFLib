"""
A module for website browsing and traffic capture. Ideally, they should work coorperatively
in an asynchronized style. The general workflow is as follows.

main        ----------------------------------------------------------------------------------------------------->
                         |      |                                                           ^       ^
browsing                 |      v-----------------------------------------------------------|       |
                         |                                                                          |
sniff                    v---------------------------------------------------------------------------
                         |                                                                          |
capture                  |--------------------------------------------------------------------------|
"""

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.firefox.service import Service

from scapy.all import sniff, wrpcap
import pyshark
from pyshark.capture.capture import Capture

import time
import threading
from typing import Union
from pathlib import Path
from urllib.parse import urlparse
import os

gecko_path = r'/usr/local/bin/geckodriver'

"""
This filter is a Capture Filter to filter the annoying traffic which, with high probability, is NOT related with the
traffic directly generated by requesting the website. Although some protocols DO relate to surfing the Web, e.h., SOAP and DNS,
they do NOT belong to the communication process between the client and server. Therefore, they are NOT included in
the capture.

The semantics of the filter is that we ONLY want TCP or UDP packets, but the following protocols are NOT considered:

LLMNR (5355), MDNS (5353), SOAP (3702), NTP (123), SSDP (1900), SSH (22), RDP (3389), DOT (853), HTTP (80)

NOTE: This filter is not exhausted, and further updates are possible in the future.
NOTE: Plain HTTP (port 80) is excluded after some consideration, since most of the request are based on HTTPS 
"""
common_filter = 'not (port 53 or port 22 or port 3389 or port 5355 or port 5353 or port 3702 or port 123 or port 1900 or port 853 or port 80) and (tcp or udp)'

def capture(url, iface, output_file, timeout=200, capture_filter=common_filter, log_output=None):
    stop_event = threading.Event()

    def _sniff(iface, output_file):
        # print("Capturing Starts.......................")
        capture = sniff(iface=iface, filter=capture_filter, stop_filter=lambda _: stop_event.is_set())
        wrpcap(output_file, capture)
        # print("Capturing Ends.......................")

    def browse(url, timeout, log_output=log_output):
        time.sleep(1) # maybe waiting for interface to be ready?
        service = Service(executable_path=gecko_path, log_output=log_output)

        options = Options()
        options.add_argument("--headless") 
        options.set_preference("browser.cache.disk.enable", False)
        options.set_preference("browser.cache.memory.enable", False)
        options.set_preference("browser.cache.offline.enable", False)
        options.set_preference("network.http.use-cache", False)

        driver = webdriver.Firefox(options=options, service=service)
        # print("Browsing Starts.......................")
        driver.get(url)
        time.sleep(timeout)
        # Notify the capture thread that the capturing process is over.
        stop_event.set()
        driver.quit()
        # print("Browsing Ends.......................")

    browse_thread = threading.Thread(target=browse, kwargs={"url": url, "timeout": timeout})
    capture_thread = threading.Thread(target=_sniff, kwargs={"iface": iface, "output_file": output_file})

    capture_thread.start()
    browse_thread.start()

    browse_thread.join()
    capture_thread.join()

def read_host_list(file) -> list:
    """
    Read the hostname list file, remove the possible duplicates, and store the results into a list.
    """
    def strip_url(url : str) -> str:
        """
        To strip possible protocol descriptors in the URL, e.g., https://.
        """
        parsed_url = urlparse(url)
        hostname = parsed_url.netloc
        return hostname
    host_list = []
    with open(file, 'r') as f:
        for line in f:
            if line not in host_list: # Avoid possible dups
                host_list.append(strip_url(line))

    return host_list

def batch_capture(base_dir, host_list, iface, 
                  capture_fileter=common_filter, 
                  repeat=20, 
                  timeout=200, 
                  log_output=None):
    """
    Capture the traffic of a list of hosts. The capturing and storing process is illustrated as follows.
    Suppose the host_list = [www.baidu.com, www.zhihu.com, www.google.com], and the base_dir is set to
    $home. Moreover, repeat is set to 2. Then, the resulting capture directory should be

    ```
    $home
      |------www.baidu.com
      |            |---------www.baidu.com_00.pcap
      |            |---------www.baidu.com_01.pcap
      |
      |------www.zhihu.com
      |            |---------www.zhihu.com_00.pcap
      |            |---------www.zhihu.com_01.pcap
      |
      |------www.google.com
      |             |---------www.google.com_00.pcap
      |             |---------www.google.com_01.pcap
    ```

    NOTE: Currently, batch_capture by default using HTTPS for requesting. So the caller needs not
    to add 'https://' before the hostname.

    Params
    ------
    base_dir : str
        The base directory to hold all captures for each hostname.

    host_list : list
        The list of hostnames to perform capture.

    iface : str
        The inferface to perform capture.

    capture_filter : str
        The capture filter using the BPF syntax to pass to tshark, common_filter is used by default.

    repeat : int
        The number of repetitive capture towards the same hostname, note that repeat should be large
        enough (>=20) to obtain a stable website fingerprint.

    timeout : int
        The amount of seconds after which the headless browser would stop. Timeout should be large
        enough for the website to load entirely.

    log_output : str
        The path for Selenium to record the debug log files.
    """
    proto_header = "https://"
    # Handle directory, create if necessary. 
    # Ref: https://stackoverflow.com/questions/273192/how-do-i-create-a-directory-and-any-missing-parent-directories

    for host in host_list:
        # Create a proper subdirectory for each host. Set parents=True to create base_dir if needed.
        # set exist_ok=True to avoid FileExistsError.
        Path("{}/{}".format(base_dir, host)).mkdir(parents=True, exist_ok=True)
        url = proto_header + host
        for i in range(repeat):
            output_file = os.path.join(base_dir, host, "{}_{:02d}.pcapng".format(host, i))
            capture(url=url, 
                    timeout=timeout, 
                    iface=iface, 
                    output_file=output_file,
                    capture_filter=capture_fileter,
                    log_output=log_output)

def SNI_extract(capture : Capture) -> set:
    """
    Extract all SNIs from a capture, and return a set that contains these SNIs.
    """
    SNIs = set()

    def process_packet(packet):
        try:
            if 'TLS' in packet:
                tls_layer = packet['TLS']
                if hasattr(tls_layer, 'handshake_extensions_server_name'):
                    SNI = tls_layer.handshake_extensions_server_name
                    SNIs.add(SNI)
        except AttributeError as e:
            # Handle packets that don't have the expected structure
            print(f"Error processing packet: {e}")

    for pkt in capture:
        process_packet(pkt)
    return SNIs

def stream_number_extract(capture : Capture, check) -> set:
    """
    Extract all TCP stream numbers for the streams where at least one packet within satisfies
    the condition required by the check.

    For example, if the check checks whether a TLS session is for SNI=www.baidu.com, it iterates
    over all the packets (all Client Hello's actually), if some packet contains the SNI, the tcp.stream
    numbers will be recorded.

    TODO: Currently, the extractor only works for TCP-based protocols. Integrating the support for UDP will
    be finished in the future. :)

    Parameter
    ---------
    check : function(pkt) -> bool
        The check on packet. Return TRUE if the packet satisfies the condition.

    Return
    ------
    set : The set contains the stream numbers each of which contains at least 1 packet satisfying check.
    """
    stream_numbers = set(pkt['TCP'].stream for pkt in capture if 'TCP' in pkt and check(pkt))
    return stream_numbers

def stream_extract_filter(stream_numbers : Union[list, set]):
    """
    Extract the streams with the given stream_numbers from input_file, and write the results to output_file.
    """
    extended_stream_numbers = ["tcp.stream == " + stream_number for stream_number in stream_numbers]
    display_filter = " or ".join(extended_stream_numbers)

    return display_filter

def stream_exclude_filter(stream_numbers : Union[list, set]):
    """
    Remove the streams with the given stream_numbers from input_file, and write the other streams to output_file.
    """
    extended_stream_numbers = ["tcp.stream != " + stream_number for stream_number in stream_numbers]
    display_filter = " and ".join(extended_stream_numbers)

    return display_filter